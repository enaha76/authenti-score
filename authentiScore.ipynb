{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "jvBY1evvwD4t",
        "outputId": "087ef05f-7307-4297-bfac-1befdeb7f036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading dataset...\n",
            "Original dataset shape: (487235, 2)\n",
            "Column names: ['text', 'generated']\n",
            "Renamed 'generated' column to 'label'\n",
            "Unique values in label column: [0 1]\n",
            "Label distribution: {0: 305797, 1: 181438}\n",
            "Reduced dataset to 1000 balanced samples\n",
            "New label distribution: {1: 500, 0: 500}\n",
            "Training set: 800 samples\n",
            "Validation set: 200 samples\n",
            "Datasets prepared successfully\n",
            "Loading model: distilbert-base-uncased\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.204200</td>\n",
              "      <td>0.219057</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.924576</td>\n",
              "      <td>0.934783</td>\n",
              "      <td>0.925000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.134100</td>\n",
              "      <td>0.057751</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.979998</td>\n",
              "      <td>0.980192</td>\n",
              "      <td>0.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.065700</td>\n",
              "      <td>0.091480</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.974994</td>\n",
              "      <td>0.975428</td>\n",
              "      <td>0.975000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 0.057750824838876724, 'eval_accuracy': 0.98, 'eval_f1': 0.97999799979998, 'eval_precision': 0.9801920768307323, 'eval_recall': 0.98, 'eval_runtime': 0.5376, 'eval_samples_per_second': 372.023, 'eval_steps_per_second': 46.503, 'epoch': 3.0}\n",
            "Saving model and tokenizer...\n",
            "Model saved to /content/drive/MyDrive/authentiscore_model\n",
            "Testing model with sample texts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: This is a human-written test sentence to check model performance.\n",
            "Prediction: AI-generated (confidence: 0.9540)\n",
            "---\n",
            "Text: The quantum mechanics underlying photosynthesis have been extensively studied.\n",
            "Prediction: AI-generated (confidence: 0.9795)\n",
            "---\n",
            "Training and evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "# AI Text Detection Model Training Script - Final Fixed Version\n",
        "# For Google Colab with T4 GPU - Authentiscore Project\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import random\n",
        "\n",
        "# Set paths - update these paths as needed\n",
        "DATA_PATH = '/content/drive/MyDrive/AI_Human.csv'  # Update this path\n",
        "OUTPUT_DIR = 'ml_models'  \n",
        "MODEL_NAME = 'distilbert-base-uncased' \n",
        "\n",
        "# Create output directory if not exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        \n",
        "    )\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "    print(f\"Column names: {df.columns.tolist()}\")\n",
        "\n",
        "    # Check if the necessary columns exist\n",
        "    if 'text' not in df.columns:\n",
        "        raise ValueError(\"Dataset must have a 'text' column\")\n",
        "\n",
        "    # Rename 'generated' column to 'label' if it exists\n",
        "    if 'generated' in df.columns:\n",
        "        # Ensure the values are proper integers (0 and 1)\n",
        "        df['generated'] = df['generated'].astype(float).astype(int)\n",
        "        df = df.rename(columns={'generated': 'label'})\n",
        "        print(\"Renamed 'generated' column to 'label'\")\n",
        "    elif 'is_ai_generated' in df.columns:\n",
        "        df = df.rename(columns={'is_ai_generated': 'label'})\n",
        "        print(\"Renamed 'is_ai_generated' column to 'label'\")\n",
        "\n",
        "    # Check if 'label' column exists after renaming\n",
        "    if 'label' not in df.columns:\n",
        "        raise ValueError(\"Could not find a 'label' column in the dataset\")\n",
        "\n",
        "    # Make sure label column contains only 0 and 1 integers\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    unique_labels = df['label'].unique()\n",
        "    print(f\"Unique values in label column: {unique_labels}\")\n",
        "\n",
        "    # Balance the dataset\n",
        "    label_counts = df['label'].value_counts()\n",
        "    print(f\"Label distribution: {label_counts.to_dict()}\")\n",
        "\n",
        "    # Limit to 1000 samples total (500 for each class)\n",
        "    samples_per_class = min(500, min(label_counts[0], label_counts[1]))\n",
        "\n",
        "    df_class_0 = df[df['label'] == 0].sample(samples_per_class, random_state=42)\n",
        "    df_class_1 = df[df['label'] == 1].sample(samples_per_class, random_state=42)\n",
        "\n",
        "    df = pd.concat([df_class_0, df_class_1], ignore_index=True)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle the data\n",
        "\n",
        "    print(f\"Reduced dataset to {len(df)} balanced samples\")\n",
        "    print(f\"New label distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or processing dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "# Split dataset into train and validation\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "print(f\"Training set: {len(train_df)} samples\")\n",
        "print(f\"Validation set: {len(val_df)} samples\")\n",
        "\n",
        "# Prepare tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set a smaller max_length for T4 GPU\n",
        "MAX_LENGTH = 256  # Reduced from 512 to save memory\n",
        "\n",
        "# Custom dataset class that correctly formats the labels\n",
        "class TextClassificationDataset(TorchDataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])  # Make sure 'labels' (not 'label') is used\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Prepare datasets using the custom dataset class\n",
        "train_dataset = TextClassificationDataset(\n",
        "    train_df['text'].tolist(),\n",
        "    train_df['label'].tolist(),\n",
        "    tokenizer,\n",
        "    MAX_LENGTH\n",
        ")\n",
        "\n",
        "val_dataset = TextClassificationDataset(\n",
        "    val_df['text'].tolist(),\n",
        "    val_df['label'].tolist(),\n",
        "    tokenizer,\n",
        "    MAX_LENGTH\n",
        ")\n",
        "\n",
        "print(\"Datasets prepared successfully\")\n",
        "\n",
        "# Load pre-trained model for sequence classification\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        ")\n",
        "\n",
        "# Define compute_metrics function\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "        \"precision\": precision[\"precision\"],\n",
        "        \"recall\": recall[\"recall\"]\n",
        "    }\n",
        "\n",
        "# Define training arguments - optimized for T4 GPU\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,  # Reduced for T4 GPU\n",
        "    per_device_eval_batch_size=8,   # Reduced for T4 GPU\n",
        "    num_train_epochs=3,             # Using fewer epochs for faster training\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        "    fp16=True,                      # Enable mixed precision training for T4 GPU\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,             # Keep only the 2 best checkpoints\n",
        "    disable_tqdm=False,             # Show progress bar\n",
        "    # Disable wandb reporting to simplify things\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Starting model training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    print(\"Saving model and tokenizer...\")\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"Model saved to {OUTPUT_DIR}\")\n",
        "\n",
        "    # Test model with sample text\n",
        "    print(\"Testing model with sample texts...\")\n",
        "    from transformers import pipeline\n",
        "\n",
        "    classifier = pipeline(\"text-classification\", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR)\n",
        "\n",
        "    test_texts = [\n",
        "        \"This is a human-written test sentence to check model performance.\",\n",
        "        \"The quantum mechanics underlying photosynthesis have been extensively studied.\"\n",
        "    ]\n",
        "\n",
        "    for text in test_texts:\n",
        "        result = classifier(text)[0]\n",
        "        label = \"AI-generated\" if result[\"label\"] == \"LABEL_1\" else \"Human-written\"\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"Prediction: {label} (confidence: {result['score']:.4f})\")\n",
        "        print(\"---\")\n",
        "\n",
        "    print(\"Training and evaluation complete!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during training or evaluation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    # Save model anyway in case of partial training\n",
        "    try:\n",
        "        trainer.save_model(OUTPUT_DIR + \"_partial\")\n",
        "        tokenizer.save_pretrained(OUTPUT_DIR + \"_partial\")\n",
        "        print(f\"Partially trained model saved to {OUTPUT_DIR}_partial\")\n",
        "    except:\n",
        "        print(\"Could not save partial model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6he0eSK3uVl",
        "outputId": "37e93699-effe-4c27-9c06-314c0c6145ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "\n",
            "=== AUTHENTISCORE DETECTION RESULTS ===\n",
            "\n",
            "Text #1: This is a human-written text about my day at the p...\n",
            "🤖 AI-GENERATED (Confidence: 95.3%)\n",
            "--------------------------------------------------\n",
            "Text #2: The implementation of neural network architectures...\n",
            "🤖 AI-GENERATED (Confidence: 99.3%)\n",
            "--------------------------------------------------\n",
            "Text #3: My grandmother's recipe for apple pie includes cin...\n",
            "🤖 AI-GENERATED (Confidence: 96.4%)\n",
            "--------------------------------------------------\n",
            "Text #4: Utilizing advanced algorithms and machine learning...\n",
            "🤖 AI-GENERATED (Confidence: 99.2%)\n",
            "--------------------------------------------------\n",
            "Text #5: I couldn't believe how the movie ended! The plot t...\n",
            "🤖 AI-GENERATED (Confidence: 94.7%)\n",
            "--------------------------------------------------\n",
            "Text #6: The quantitative analysis demonstrates a statistic...\n",
            "🤖 AI-GENERATED (Confidence: 99.3%)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Simple test script for Authentiscore Model\n",
        "# Run this after training to test your model on new texts\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load your model\n",
        "MODEL_PATH = '/content/drive/MyDrive/authentiscore_modelb'\n",
        "\n",
        "# Create the classifier pipeline\n",
        "try:\n",
        "    classifier = pipeline(\"text-classification\", model=MODEL_PATH, tokenizer=MODEL_PATH)\n",
        "    print(\"Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Trying to load partial model instead...\")\n",
        "    try:\n",
        "        classifier = pipeline(\"text-classification\",\n",
        "                             model=MODEL_PATH + \"_partial\",\n",
        "                             tokenizer=MODEL_PATH + \"_partial\")\n",
        "        print(\"Partial model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading partial model: {e}\")\n",
        "        raise\n",
        "\n",
        "# Test texts (add your own examples)\n",
        "test_texts = [\n",
        "    \"This is a human-written text about my day at the park. I saw birds and enjoyed the sunshine.\",\n",
        "    \"The implementation of neural network architectures facilitates the optimization of computational resources while maintaining performance metrics within acceptable parameters.\",\n",
        "    \"My grandmother's recipe for apple pie includes cinnamon, sugar, and a secret ingredient she never revealed to anyone.\",\n",
        "    \"Utilizing advanced algorithms and machine learning techniques, the system processes large volumes of data to extract meaningful patterns and insights.\",\n",
        "    \"I couldn't believe how the movie ended! The plot twist was completely unexpected and left me speechless.\",\n",
        "    \"The quantitative analysis demonstrates a statistically significant correlation between the variables, suggesting a causal relationship worthy of further investigation.\"\n",
        "]\n",
        "\n",
        "# Process each text and display results\n",
        "print(\"\\n=== AUTHENTISCORE DETECTION RESULTS ===\\n\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    result = classifier(text)[0]\n",
        "    confidence = result['score'] * 100\n",
        "\n",
        "    if result[\"label\"] == \"LABEL_1\":\n",
        "        detection = \"AI-GENERATED\"\n",
        "        emoji = \"🤖\"\n",
        "    else:\n",
        "        detection = \"HUMAN-WRITTEN\"\n",
        "        emoji = \"👤\"\n",
        "\n",
        "    print(f\"Text #{i+1}: {text[:50]}...\" if len(text) > 50 else f\"Text #{i+1}: {text}\")\n",
        "    print(f\"{emoji} {detection} (Confidence: {confidence:.1f}%)\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9jFIlqEZK0B",
        "outputId": "fece7f33-d40d-4e0a-883a-bc66e16c011f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.21.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully\n",
            "ONNX version: 1.17.0\n",
            "ONNX Runtime version: 1.21.0\n",
            "Loading model from: /content/drive/MyDrive/authentiscore_model\n",
            "Loading tokenizer...\n",
            "Loading model...\n",
            "Model and tokenizer loaded successfully\n",
            "Creating dummy input for ONNX export...\n",
            "Exporting model to ONNX format with opset version 14...\n",
            "Saving tokenizer files to: /content/drive/MyDrive/authentiscore_model/tokenizer\n",
            "Verifying ONNX model...\n",
            "ONNX model verification successful\n",
            "Conversion complete!\n",
            "Model exported to ONNX format and saved at: /content/drive/MyDrive/authentiscore_model/model.onnx\n",
            "Tokenizer files saved at: /content/drive/MyDrive/authentiscore_model/tokenizer\n",
            "ONNX model file size: 255.54 MB\n",
            "\n",
            "Next steps:\n",
            "1. Download the model.onnx file and tokenizer folder from your Google Drive\n",
            "2. Use them with the ONNX Runtime FastAPI application locally\n"
          ]
        }
      ],
      "source": [
        "# Convert PyTorch model to ONNX format for lightweight deployment\n",
        "# For Google Colab\n",
        "\n",
        "# First, install required packages\n",
        "!pip install onnx onnxruntime\n",
        "\n",
        "# Import system libraries\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully\")\n",
        "\n",
        "# Verify ONNX installation\n",
        "try:\n",
        "    import onnx\n",
        "    print(f\"ONNX version: {onnx.__version__}\")\n",
        "    import onnxruntime\n",
        "    print(f\"ONNX Runtime version: {onnxruntime.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing ONNX: {e}\")\n",
        "    print(\"Please restart the runtime after package installation and run this script again.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Import transformer libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Set model path\n",
        "model_path = '/content/drive/MyDrive/authentiscore_model'\n",
        "print(f\"Loading model from: {model_path}\")\n",
        "\n",
        "# Verify model path exists\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(f\"Model directory not found at {model_path}. Please check the path.\")\n",
        "\n",
        "try:\n",
        "    # Load the trained model and tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    print(\"Model and tokenizer loaded successfully\")\n",
        "\n",
        "    # Create dummy input for tracing\n",
        "    print(\"Creating dummy input for ONNX export...\")\n",
        "    dummy_input = tokenizer(\"This is a sample text\", return_tensors=\"pt\",\n",
        "                            truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "    # Define export path\n",
        "    onnx_path = os.path.join(model_path, \"model.onnx\")\n",
        "\n",
        "    # Export to ONNX format\n",
        "    print(\"Exporting model to ONNX format with opset version 14...\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        (dummy_input.input_ids, dummy_input.attention_mask),\n",
        "        onnx_path,\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"logits\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\"},\n",
        "            \"attention_mask\": {0: \"batch_size\"},\n",
        "            \"logits\": {0: \"batch_size\"}\n",
        "        },\n",
        "        opset_version=14\n",
        "    )\n",
        "\n",
        "    # Save tokenizer files to a specific directory\n",
        "    tokenizer_path = os.path.join(model_path, \"tokenizer\")\n",
        "    print(f\"Saving tokenizer files to: {tokenizer_path}\")\n",
        "    tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "    # Verify the ONNX model\n",
        "    print(\"Verifying ONNX model...\")\n",
        "    onnx_model = onnx.load(onnx_path)\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "    print(\"ONNX model verification successful\")\n",
        "\n",
        "    print(\"Conversion complete!\")\n",
        "    print(f\"Model exported to ONNX format and saved at: {onnx_path}\")\n",
        "    print(f\"Tokenizer files saved at: {tokenizer_path}\")\n",
        "\n",
        "    # Verify exported model file size\n",
        "    onnx_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "    print(f\"ONNX model file size: {onnx_size_mb:.2f} MB\")\n",
        "\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Download the model.onnx file and tokenizer folder from your Google Drive\")\n",
        "    print(\"2. Use them with the ONNX Runtime FastAPI application locally\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during model conversion: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
